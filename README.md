# Tesla Stock Price Prediction using Machine Learning in Python

This project predicts **next-day stock price movement of Tesla (TSLA)** using
Machine Learning models in Python. It focuses on **classification** of whether
the stock will go **up or down tomorrow**, based on historical OHLC data and
engineered features.

The project was developed as part of my **MCA Major Project** at **Auroraâ€™s PG College (MCA)** and implemented during my internship at **Sansah Innovations Pvt. Ltd.**

---

## ðŸš€ Project Overview

- Problem: Predict if **tomorrow's closing price** of Tesla will be **higher**
  than today's closing price.
- Type: **Binary classification** (Up = 1, Down = 0)
- ML Techniques:
  - **StandardScaler** for feature normalization
  - **Logistic Regression** as baseline model
  - **SVC (Polynomial kernel)** for non-linear separation
  - **XGBClassifier (XGBoost)** as advanced ensemble model
- Frontend: **Streamlit** web app for user-friendly predictions

The system takes basic OHLC inputs and quarter-end information and predicts
whether the Tesla stock is likely to go **up** or **down** on the next trading day.

---

## ðŸ§  Machine Learning Approach

### 1. Features

From each daily record (OHLC data):

- `open-close` = Open âˆ’ Close  
- `low-high`   = Low âˆ’ High  
- `is_quarter_end` = 1 if month % 3 == 0 else 0  

**Target variable:**

- `target = 1` if next day Close > today Close  
- `target = 0` otherwise

### 2. Preprocessing

- Remove unnecessary columns (e.g., `Adj Close`)
- Extract `day`, `month`, `year` from `Date`
- Handle missing values / duplicates
- **Standardize** numerical features using `StandardScaler` so that each
  feature has mean 0 and standard deviation 1.

### 3. Models

- **Logistic Regression**
  - Baseline model
  - Interpretable and fast
- **SVC (poly kernel)**
  - Captures non-linear decision boundary
- **XGBClassifier**
  - Gradient boosting ensemble model
  - Handles non-linearity and complex feature interactions
  - Uses regularization to reduce overfitting

### 4. Evaluation Metrics

- Accuracy
- Precision, Recall, F1-Score
- ROC-AUC
- Confusion Matrix
- Time-series comparison of predicted vs actual movements

In experiments, **XGBClassifier** achieved higher accuracy
(around **70â€“75%**) compared to baseline Logistic Regression
(around **62â€“65%**), with better ROC-AUC and F1-score.

---

## ðŸ“‚ Project Structure

```text
.
â”œâ”€ app/
â”‚   â””â”€ app.py               # Streamlit UI for interactive predictions
â”‚
â”œâ”€ src/
â”‚   â”œâ”€ train_model.py       # Training pipeline for ML models
â”‚   â””â”€ utils.py             # Helper functions (preprocessing, feature engineering)
â”‚
â”œâ”€ models/
â”‚   â””â”€ xgb_model.pkl        # Saved XGBoost model
â”‚
â”œâ”€ data/
â”‚   â””â”€ Tesla.csv            # Historical Tesla stock data
â”‚
â”œâ”€ notebooks/
â”‚   â””â”€ eda_and_models.ipynb # EDA + experimentation (optional)
â”‚
â”œâ”€ README.md
â”œâ”€ requirements.txt
â””â”€ .gitignore
